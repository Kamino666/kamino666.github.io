

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head><!-- hexo injector head_begin start -->
  <script src="/js/Chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js"></script>
<!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/avatar.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="Pytorch的DDP指的是DistributedDataParallel，位于torch.nn.parallel中，用于多GPU的模型训练。相比于之前的DP，DDP的速度快了很多。DDP支持多卡多机器，但我没有多机器，所以本文针对最常用的单机器多卡。">
  <meta name="author" content="Kamino">
  <meta name="keywords" content="Kamino">
  <meta name="description" content="Pytorch的DDP指的是DistributedDataParallel，位于torch.nn.parallel中，用于多GPU的模型训练。相比于之前的DP，DDP的速度快了很多。DDP支持多卡多机器，但我没有多机器，所以本文针对最常用的单机器多卡。">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch DDP使用方法以及注意点">
<meta property="og:url" content="http://blog.kamino.link/2021/12/09/Pytorch%20DDP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9/index.html">
<meta property="og:site_name" content="Kamino&#39;s Blog">
<meta property="og:description" content="Pytorch的DDP指的是DistributedDataParallel，位于torch.nn.parallel中，用于多GPU的模型训练。相比于之前的DP，DDP的速度快了很多。DDP支持多卡多机器，但我没有多机器，所以本文针对最常用的单机器多卡。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20211209210301.png">
<meta property="article:published_time" content="2021-12-09T13:00:00.000Z">
<meta property="article:modified_time" content="2022-07-21T09:09:28.426Z">
<meta property="article:author" content="Kamino">
<meta property="article:tag" content="Pytorch">
<meta property="article:tag" content="DDP">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20211209210301.png">
  
  <title>Pytorch DDP使用方法以及注意点 - Kamino&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"blog.kamino.link","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"4a5b4b79f797585f54a6f176ffa56438","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Kamino</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://kamino-img.oss-cn-beijing.aliyuncs.com/20211020151909.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Pytorch DDP使用方法以及注意点">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-12-09 21:00" pubdate>
        2021年12月9日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.1k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      13 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Pytorch DDP使用方法以及注意点</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2022年7月21日 下午
                
              </p>
            
            <div class="markdown-body">
              <h1>Pytorch DDP使用方法以及注意点</h1>
<p>[TOC]</p>
<p>Pytorch的DDP指的是<code>DistributedDataParallel</code>，位于<code>torch.nn.parallel</code>中，用于多GPU的模型训练。相比于之前的DP，DDP的速度快了很多。DDP支持多卡多机器，但我没有多机器，所以本文针对最常用的单机器多卡。</p>
<h2 id="原理">原理</h2>
<p>DDP加速的原理是通过启动多个<strong>进程</strong>，提高同时训练的<strong>batch size</strong>来增加并行度的，每一个进程都会加载一个模型，用<strong>不同的数据</strong>进行训练之后得到各自的梯度，然后通过<strong>Ring-Reduce</strong>算法获得所有进程的梯度，然后进行相同的梯度下降。注意在训练前和训练后，所有进程的模型参数都是同步了的。</p>
<p>Ring-Reduce是很简单理解的一个算法：每个进程都从左手边获得一份梯度，然后从右手发送一份梯度（一份指的是一个GPU得出的梯度），经过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>次迭代之后，所有进程都获得了相同的完整的梯度。如下图，假设梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∇</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\nabla w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>是GPU0算出来的，第一次<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∇</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\nabla w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>被发送到GPU1，第二次被GPU1发送到GPU2，第三次被GPU2发送到GPU3，第四次被发送到GPU4，第五次被发回给GPU0。这样每个进程都只需要接收一个，发送一个，而且能够清楚知道什么时候结束。</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20211209210301.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="基础概念">基础概念</h2>
<p>先来了解基础概念：</p>
<ul>
<li><code>world_size</code>：并行数，即总共用的卡数。</li>
<li><code>rank</code>：当前进程全局序号，范围0~world_size。</li>
<li><code>local_rank</code>：本地序号，由于本文介绍单机器，所以和上面一样。</li>
<li><code>master_port</code>：DDP需要进程间传递数据，所以需要使用端口。</li>
<li><code>master_address</code>：同上，需要使用IP地址。</li>
</ul>
<h2 id="使用方法">使用方法</h2>
<p>基本逻辑是这样：</p>
<pre><code class=" mermaid">graph
A[初始化多进程并获取rank号]--&gt;B[使用DistributedSampler提供数据]
B --&gt; C[将数据和模型都放在GPU上]
C --&gt; D[将模型用DDP包裹]
D --&gt; E[训练]
E --&gt; F[loss.backward同步梯度]
F --&gt; G[保存参数]
</code></pre>
<h3 id="初始化多进程、获取rank号">初始化多进程、获取rank号</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用这种方法获取local_rank</span><br><span class="hljs-comment"># 其他获取local_rank的方法已经被弃用</span><br>local_rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])<br><span class="hljs-comment"># 设置device</span><br>torch.cuda.set_device(local_rank)<br><span class="hljs-comment"># 用nccl后端初始化多进程，一般都用这个</span><br>dist.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>)<br><span class="hljs-comment"># 获取device，之后的模型和张量都.to(device)</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>, local_rank)<br></code></pre></td></tr></table></figure>
<h3 id="数据">数据</h3>
<p>对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>张卡上的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>个模型，我们当然希望将数据分成不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>部分，分别送给它们训练，所以需要使用<code>torch.utils.data.distributed.DistributedSampler</code>帮助我们自动分配数据。</p>
<blockquote>
<p>**注意！只有训练集才要用Sampler！**在train之后，经常使用验证集对数据集进行验证得到validation_loss，此时没有必要使用多卡，只需要在一个进程上进行验证。</p>
<p>在多卡模式下要进行只在一个进程上的操作，通过<code>model.module(inputs)</code>而不是<code>model(inputs)</code>来调用<code>forward()</code>前向传播，而其他进程通过<code>torch.distributed.barrier()</code>来等待主进程完成validate操作。</p>
<p>假如要多卡推理，参考这篇文章写一个新的sampler[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250471767">原创][深度][PyTorch] DDP系列第三篇：实战与技巧 - 知乎 (zhihu.com)</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataset = MyDataset()<br><span class="hljs-comment"># shuffle不在dataloader中设置，而是在sampler中设置</span><br>train_sampler = DistributedSampler(train_dataset, shuffle=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># batch_size指的是一张卡上的batch_size，总batch_size应该是要乘并行数</span><br>train_loader = torch.utils.data.DataLoader(train_dataset, <br>                                           batch_size=<span class="hljs-number">64</span>, <br>                                           sampler=train_sampler)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-comment"># 每轮开始前需要调用这个方法进行正确的数据shuffle</span><br>    <span class="hljs-comment"># 否则每一轮用的都是相同的顺序</span><br>    train_sampler.set_epoch(epoch)<br>    ...<br></code></pre></td></tr></table></figure>
<h3 id="把数据和模型放在GPU上">把数据和模型放在GPU上</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 直接to(device)即可，注意要收到返回值</span><br>model = YourModel()<br>model = model.to(device)<br>your_data = your_data.to(device)<br></code></pre></td></tr></table></figure>
<h3 id="把模型用DDP包裹">把模型用DDP包裹</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># local_rank就是进程号</span><br><span class="hljs-comment"># 从此之后，调用model()就是用DDP模式的前向传播</span><br><span class="hljs-comment"># 要使用原始的前向传播，需要model.module()</span><br>model = DDP(model, device_ids=[local_rank], output_device=local_rank)<br></code></pre></td></tr></table></figure>
<h3 id="训练、同步梯度">训练、同步梯度</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 仍然可以直接调用模型的train()方法</span><br><span class="hljs-comment"># 但是假如要调用其他你自己写的方法，就得model.module.func()</span><br>model.train()<br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    loss = model(data)<br>    optimizer.zero_grad()<br>    loss.backward()  <span class="hljs-comment"># 这个操作自动同步梯度</span><br>    optimizer.step()<br>    <span class="hljs-comment"># 但是仍然需要累加得到所有进程loss的值的和</span><br>    dist.all_reduce(loss, op=dist.ReduceOp.SUM)<br>    <span class="hljs-comment"># 然后除以并行数，就是这个batch的loss值了</span><br>    loss /= world_size<br></code></pre></td></tr></table></figure>
<h3 id="保存参数">保存参数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存的是参数，不需要DDP包裹</span><br>torch.save(model.module.state_dict())<br></code></pre></td></tr></table></figure>
<h3 id="运行">运行</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 通过外部命令运行</span> <br><span class="hljs-meta">#</span><span class="bash"> 通过CUDA_VISIBLE_DEVICES控制可见的卡数</span><br><span class="hljs-meta">#</span><span class="bash"> 通过--nproc_per_node确定使用多少卡</span><br>CUDA_VISIBLE_DEVICES=&quot;0,1,2,3&quot; python -m torch.distributed.run --nproc_per_node 4 train.py <br></code></pre></td></tr></table></figure>
<h2 id="DDP注意点复习！">DDP注意点复习！</h2>
<ol>
<li>要把模型和数据放在进程对应的那张卡上</li>
<li>要使用Sampler来分发训练数据，并且shuffle不设置在Dataloder中而是Sampler中，每个epoch还需要调用Sampler的<code>set_epoch()</code>方法。</li>
<li>训练和验证区分较大，验证一般在主进程中进行一次验证即可，不需要sampler，操作和单卡一样，之后将数据同步给其他进程。</li>
<li>在多卡时要调用模型的其他方法或者使用单卡的模式，需要用<code>model.module</code>来获得原始模型，同样保存参数时也保存的是<code>model.module</code>的参数而不是DDP包裹的。</li>
</ol>
<h2 id="DDP小技巧">DDP小技巧</h2>
<h3 id="数据同步">数据同步</h3>
<p>使用<code>dist.all_reduce(loss, op=dist.ReduceOp.SUM)</code>可以同步tensor的数据，由于算法限制，要算平均值只能用求和运算<code>dist.ReduceOp.SUM</code>之后再除以<code>world_size</code>。</p>
<p>假如要同步的不是tensor，可以创建Tensor然后<strong>放进对应的GPU</strong>，再同步。</p>
<p>假如需要获得每个进程的某个tensor的值（即有n个GPU就获得n个值），那么使用<code>dist.all_gather</code>可以获得tensor列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 官网API文档</span><br><span class="hljs-comment"># All tensors below are of torch.int64 dtype.</span><br><span class="hljs-comment"># We have 2 process groups, 2 ranks.</span><br>tensor_list = [torch.zeros(<span class="hljs-number">2</span>, dtype=torch.int64) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]<br>tensor_list<br><span class="hljs-comment"># [tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1</span><br>tensor = torch.arange(<span class="hljs-number">2</span>, dtype=torch.int64) + <span class="hljs-number">1</span> + <span class="hljs-number">2</span> * rank<br>tensor<br><span class="hljs-comment"># tensor([1, 2]) # Rank 0</span><br><span class="hljs-comment"># tensor([3, 4]) # Rank 1</span><br>dist.all_gather(tensor_list, tensor)<br>tensor_list<br><span class="hljs-comment"># [tensor([1, 2]), tensor([3, 4])] # Rank 0</span><br><span class="hljs-comment"># [tensor([1, 2]), tensor([3, 4])] # Rank 1</span><br></code></pre></td></tr></table></figure>
<p>同步数据时假如要控制所有进程同时，可以使用<code>torch.distributed.barrier()</code>，让快的进程等一下慢的进程，假如timeout了，可以看看代码是否能优化，或者在运行之前提供参数提高timeout的值。</p>
<p><strong>假如dist.barrier()失效，可能是这种情况</strong><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/distributeddataparallel-barrier-doesnt-work-as-expected-during-evaluation/99867">DistributedDataParallel barrier doesn’t work as expected during evaluation - distributed - PyTorch Forums</a></p>
<blockquote>
<p>参考文献：</p>
<p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">原创][深度][PyTorch] DDP系列第一篇：入门教程 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/57799212/answer/612786337"> ring allreduce和tree allreduce的具体区别是什么？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/distributeddataparallel-barrier-doesnt-work-as-expected-during-evaluation/99867">DistributedDataParallel barrier doesn’t work as expected during evaluation - distributed - PyTorch Forums</a></p>
</blockquote>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/PyTorch/">PyTorch</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Pytorch/">Pytorch</a>
                    
                      <a class="hover-with-bg" href="/tags/DDP/">DDP</a>
                    
                      <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/12/23/%E6%A8%A1%E4%BB%BF%E7%8A%AF%E4%B9%A6%E8%AF%84/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">《模仿犯》——宫部美雪 ：娓娓道来的社会派悬疑小说</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Normalization/">
                        <span class="hidden-mobile">深度学习中的Normalization</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'Kamino666/kamino666.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8.8.3/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?4a5b4b79f797585f54a6f176ffa56438";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
