

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head><!-- hexo injector head_begin start -->
  <script src="/js/Chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.1/dist/jquery.min.js"></script>
<!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/avatar.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="Salesforce团队于2023年1月发布的大规模视觉-语言预训练模型，在前作BLIP的基础上发展而来，BLIP2展示了一种利用已有的大型图像编码器（如CLIP）和大型语言模型（如OPT、GPT）的训练方式，其中这两个模型在训练时均不更新参数，而是只学习连接两者的一个仅有186M参数的Q-Former。这种架构能够适应并利用如今的各种图像编码器和语言模型，并且由于冻结了大部分参数，在计算上也有巨">
  <meta name="author" content="Kamino">
  <meta name="keywords" content="Kamino">
  <meta name="description" content="Salesforce团队于2023年1月发布的大规模视觉-语言预训练模型，在前作BLIP的基础上发展而来，BLIP2展示了一种利用已有的大型图像编码器（如CLIP）和大型语言模型（如OPT、GPT）的训练方式，其中这两个模型在训练时均不更新参数，而是只学习连接两者的一个仅有186M参数的Q-Former。这种架构能够适应并利用如今的各种图像编码器和语言模型，并且由于冻结了大部分参数，在计算上也有巨">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记 BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models">
<meta property="og:url" content="http://blog.kamino.link/2023/02/17/BLIP2/index.html">
<meta property="og:site_name" content="Kamino&#39;s Blog">
<meta property="og:description" content="Salesforce团队于2023年1月发布的大规模视觉-语言预训练模型，在前作BLIP的基础上发展而来，BLIP2展示了一种利用已有的大型图像编码器（如CLIP）和大型语言模型（如OPT、GPT）的训练方式，其中这两个模型在训练时均不更新参数，而是只学习连接两者的一个仅有186M参数的Q-Former。这种架构能够适应并利用如今的各种图像编码器和语言模型，并且由于冻结了大部分参数，在计算上也有巨">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217124428.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217124647.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217140102.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217141118.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217141518.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230301120437.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217142319.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217142655.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217143527.png">
<meta property="og:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217143108.png">
<meta property="article:published_time" content="2023-02-17T04:21:00.000Z">
<meta property="article:modified_time" content="2023-03-01T08:04:31.980Z">
<meta property="article:author" content="Kamino">
<meta property="article:tag" content="大规模视觉语言预训练">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217124428.png">
  
  <title>论文笔记 BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - Kamino&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"blog.kamino.link","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"4a5b4b79f797585f54a6f176ffa56438","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Kamino</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://kamino-img.oss-cn-beijing.aliyuncs.com/20211020151909.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="论文笔记 BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-02-17 12:21" pubdate>
        2023年2月17日 中午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.4k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      17 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">论文笔记 BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2023年3月1日 下午
                
              </p>
            
            <div class="markdown-body">
              <h1>论文笔记 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h1>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (arxiv.org)</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">LAVIS/projects/blip2 at main · salesforce/LAVIS (github.com)</a></p>
<p>Demo：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Salesforce/BLIP2">HuggingFace</a>、<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">Colab（免费资源不够直接运行）</a></p>
<p><strong>Salesforce</strong>团队于2023年1月发布的大规模视觉-语言预训练模型，在前作<a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v162/li22n.html">BLIP</a>的基础上发展而来，BLIP2展示了一种<strong>利用已有的大型图像编码器（如CLIP）和大型语言模型（如OPT、GPT）<strong>的训练方式，其中这两个模型</strong>在训练时均不更新参数，而是只学习连接两者的一个仅有186M参数的Q-Former</strong>。这种架构能够适应并利用如今的各种图像编码器和语言模型，并且由于冻结了大部分参数，在计算上也有巨大优势。BLIP2最终在各个方面都取得了SOTA的表现，并且优秀的架构设计能适应更多种的应用。</p>
<h2 id="介绍">介绍</h2>
<p>近年来视觉-语言预训练（VLP）的研究发展迅速，CLIP、ALBEF、BLIP、SimVLM、BEiT、Flamingo等都是很优秀的模型，然而在预训练中的复杂计算仍然是一个问题。视觉-语言的研究正如其名是联系视觉和语言的研究，所以很自然会想到：能不能将视觉上的单模态模型和语言上的单模态模型结合在一起？</p>
<p>BLIP2就是结合两个方面各自的单模态模型的一个工作，具有<strong>高通用性</strong>和<strong>高计算效率</strong>的特点。</p>
<p>众所周知，跨模态训练中跨模态对齐很重要，然而由于计划将语言模型参数冻结，而其并没有见到过视觉的数据，所以这种对齐变得十分困难。已有的方法（Frozen、Flamingo）使用了图像到文本的生成损失，本文认为这不足以弥补模态差距。</p>
<h2 id="模型架构">模型架构</h2>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217124428.png" srcset="/img/loading.gif" lazyload alt="BLIP2"></p>
<p>上图为BLIP-2的架构，BLIP-2在冻结的图像编码器和语言模型间加入了一个Querying Transformer，训练也被分成了两个阶段，第一阶段是利用预训练图像编码器的视觉-语言表征学习，第二阶段是利用两个冻结模型的视觉-语言生成学习。</p>
<h3 id="Q-Former">Q-Former</h3>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217124647.png" srcset="/img/loading.gif" lazyload alt="BLIP2"></p>
<p>Q-Former的具体结构如上图，其包含了一个<strong>共享参数的Self-Attention</strong>、一个Cross-Attention和<strong>两个不同的前向传播层</strong>，类似的结构在前作BLIP中也有体现，他们发现在Transformer中，不同模态的SA层可以共享，而前向传播层分开更好。</p>
<p>模型会学习32个768维的Query，即输入进Q-Former左边SA层的数据，这些token将通过CA层获取到视觉信息，并通过SA层于语言进行交互。从下往上叙述，SA层就是普通的SA，但是使用了三种Attention Mask：双向mask、多模态因果mask、单模态mask。</p>
<ol>
<li>双向mask就是没有mask，每一个输入都能对其它输入施加注意力。这个mask用作之后的Image-Text Matching任务。</li>
<li>多模态因果mask就是query只能对query施加注意力，文本端则是做Language Modeling用的因果mask，所以对应的是Image-Grounded Text Generation任务。</li>
<li>单模态mask就是每个模态只能对自己模态施加注意力，之后用在Image-Text Contrastive Learning任务上。</li>
</ol>
<p>CA层query将会学习到图像的知识，此时原本图像的很长的token序列将会被减少到32个，在这个层面也浓缩了信息，降低了计算复杂度。<strong>要注意CA层是间隔插入的，比如第0、2、4层。</strong></p>
<p>Feed Forward层根据BLIP之前的研究包含了主要的参数，所以每个模态用独立的前向层更优。</p>
<p><strong>算上32个768维的query，总共有188M可学习参数。</strong></p>
<blockquote>
<p>一些小细节：</p>
<ol>
<li>模型的三个任务要前向传播三次</li>
<li>论文中使用的Image Encoder是ViT-G（1408维）和ViT-L（1024维），官方只开源了ViT-G的版本，但Q-former主体是768维的，不同维度的特征只会在CA层变成768维。</li>
</ol>
</blockquote>
<h3 id="第一阶段训练：视觉-语言表征学习">第一阶段训练：视觉-语言表征学习</h3>
<p>此阶段进行三个任务：</p>
<ol>
<li>Image-Text Contrastive Learning（ITC）和CLIP类似，视觉侧通过单模态mask来避免了信息泄露，由于视觉测没有<code>[CLS]</code>token，所以会选择32个query中与文本<code>[CLS]</code>相似度最高的那个来进行对比学习。</li>
<li>Image-grounded Text Generation（ITG）是生成式的训练，这里用到的多模态因果mask和UniLM的设计类似，文本的输入额外添加一个<code>[DEC]</code>token来表示解码开始。</li>
<li>Image-Text Matching（ITM）是细粒度的对齐，判断图像和文本是否匹配的二分类任务，这个过程中32个query会得到多模态的信息，然后每个query通过一个二分类线性分类器得到logit，再平均32个得到最终的分数。这里和其他文献一样会采用hard negative mining策略。</li>
</ol>
<h3 id="第二阶段训练：视觉-语言生成学习">第二阶段训练：视觉-语言生成学习</h3>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217140102.png" srcset="/img/loading.gif" lazyload alt="BLIP2第二阶段训练"></p>
<p>第二阶段还会引入一个冻结参数的LLM，目前的LLM分成只使用解码器的（GPT）和编解码器都用了的（T5）模型，如图是两种训练的方法。对于基于解码器的LLM，经过Q-Former的query通过全连转换维度，然后进行LM训练；对于基于编解码器的LLM，全连后进行前缀LM训练，就是会额外添加前缀文本。</p>
<p>这个阶段中，之前学习到的query被当做了LLM的soft visual prompts，用来控制LLM的输出。</p>
<h2 id="模型训练">模型训练</h2>
<p>训练的数据集使用了129M的图像数据，数据处理和前作BLIP一样，并利用前作的方法来过滤数据。</p>
<p>Q-Former使用BERT-base初始化，图像编码器使用了CLIP和EVA-CLIP（提取特征时，去掉了ViT的最后一层），语言模型使用了OPT（仅解码器）和FlanT5（编解码器）。</p>
<p>使用16张A100（40G），最大的模型在第一阶段训练6天，第二阶段训练3天。</p>
<h3 id="实验分析">实验分析</h3>
<h3 id="Zero-shot">Zero-shot</h3>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217141118.png" srcset="/img/loading.gif" lazyload alt="表1"></p>
<p>如图，BLIP-2既开源、效果又好，而且徐娜林参数还特别少（特别是和边上那个10.2B相比）。BLIP-2全面支持prompt的应用方式，文本的prompt就添加在LLM的visual prompt后面就行：</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217141518.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p>上图这些示例的效果还是很不错的，可以在HuggingFace和Notebook中自己尝试，实测速度还是很快的。</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230301120437.png" srcset="/img/loading.gif" lazyload alt="消融"></p>
<p>替换视觉编码器和语言模型的结果如上，实验说明更优秀的视觉编码器和语言模型能够带来更好的效果，OK-VQA数据集上的效果没必过Flamingo80B的原因应该是其使用的70B参数的Chinchilla语言模型有更多的知识。</p>
<h3 id="Image-Captioning">Image Captioning</h3>
<p>进行图像描述时，本文微调了图像编码器和Q-Former来得到最好的效果，这一块在NoCaps上表现不错，但是在COCO上不是最好，但也没差多少。Finetune的成本还是略高。</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217142319.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="VQA">VQA</h3>
<p>同样微调图像编码器和Q-Former，和其他牛逼模型一样，将VQA看作是生成式任务，效果很不错，但能微调语言模型的那些模型更好一点，可能是因为数据集的限制，生成的文本要完全符合才算上正确率。</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217142655.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217143527.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h3 id="跨模态检索">跨模态检索</h3>
<p>跨模态检索不需要语言模型，所以使用图像编码器和Q-former进行微调。和其他使用了ITC、ITM的方法一样，先ITC选出前k个，然后使用ITM来进一步判断。</p>
<p>微调时进行了ITC+ITM+ITG三个任务，虽然用不到ITG，但是消融实验证明加上ITG效果更好。</p>
<p><img src="https://kamino-img.oss-cn-beijing.aliyuncs.com/20230217143108.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="BLIP-2的限制">BLIP-2的限制</h2>
<p>BLIP-2对于in-context的VQA性能不佳，可能是因为训练数据集缺少类似数据。作者说未来会建立个和Flamingo的M3W数据集类似的数据集。</p>
<blockquote>
<p>in-context就是考虑上下文的VQA，会考虑之前的提问和回答。</p>
</blockquote>
<p>另外BLIP-2没改掉LLM喜欢瞎编的毛病。</p>
<h2 id="🌀参数分析">🌀参数分析</h2>
<h3 id="Q-former">Q-former</h3>
<p>下面我把Q-former的第一层参数列出来，删除了<code>Qformer.bert.encoder.layer.0</code>的前缀。</p>
<p>可以看到，第一层包含自注意力（attention）和互注意力（crossattetntion），每个注意力层包含query、key、value、layernorm和一个融合多头的dense（但是这里并没有多头），其中互注意力层的key和value由于是视觉侧过来的，要维度变化。</p>
<p>而之后的intermediate就是Feed forward层，先升到3072，再降回768，由于有两个模态，所以也有两个intermediate。</p>
<p>这样的一层有15.16M参数，没有互注意力的层有11.81M参数，每个模态的前项传播层为4.72M参数。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.query</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.query</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.key</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.key</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.value</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.value</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.attention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.query</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.query</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.key</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 1408]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.key</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.value</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 1408]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.self</span><span class="hljs-selector-class">.value</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.crossattention</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><br><span class="hljs-number">0</span><span class="hljs-selector-class">.intermediate</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3072, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.intermediate</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3072]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 3072]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.intermediate_query</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3072, 768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.intermediate_query</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3072]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output_query</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768, 3072]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output_query</span><span class="hljs-selector-class">.dense</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output_query</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.weight</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br><span class="hljs-number">0</span><span class="hljs-selector-class">.output_query</span><span class="hljs-selector-class">.LayerNorm</span><span class="hljs-selector-class">.bias</span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[768]</span>)<br></code></pre></td></tr></table></figure>
<h3 id="EVA-CLIP-ViT-G">EVA CLIP ViT-G</h3>
<p>BLIP2用的图像编码器backbone，来自CVPR2023的论文，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.07636">EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</a>，<a target="_blank" rel="noopener" href="https://github.com/baaivision/EVA">代码链接</a></p>
<p>参数1.0B……🤔，论文用的fp16。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
                    
                      <a class="hover-with-bg" href="/categories/%E5%85%88%E8%BF%9B%E6%A8%A1%E5%9E%8B%E9%80%9F%E8%A7%88/">先进模型速览</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83/">大规模视觉语言预训练</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/01/XCLIP/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">论文笔记 XCLIP Expanding Language-Image Pretrained Models for General Video Recognition</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/02/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0Zero-Shot%20Scene%20Graph%20Relation%20Prediction%20through%20Commonsense%20Knowledge%20Integration/">
                        <span class="hidden-mobile">论文笔记 Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'Kamino666/kamino666.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8.8.3/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?4a5b4b79f797585f54a6f176ffa56438";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
